<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-12-27T08:18:04+08:00</updated><id>/feed.xml</id><title type="html">QEMU</title><subtitle>QEMU is a FAST! processor emulator
</subtitle><entry><title type="html">QEMU version 7.2.0 released</title><link href="/2022/12/15/qemu-7-2-0/" rel="alternate" type="text/html" title="QEMU version 7.2.0 released" /><published>2022-12-15T07:53:00+08:00</published><updated>2022-12-15T07:53:00+08:00</updated><id>/2022/12/15/qemu-7-2-0</id><content type="html" xml:base="/2022/12/15/qemu-7-2-0/"><![CDATA[<p>We’d like to announce the availability of the QEMU 7.2.0 release. This release contains 1800+ commits from 205 authors.</p>

<p>You can grab the tarball from our <a href="https://www.qemu.org/download/#source">download page</a>. The full list of changes are available <a href="https://wiki.qemu.org/ChangeLog/7.2">in the Wiki</a>.</p>

<p>Highlights include:</p>

<ul>
  <li>ARM: emulation support for the following CPU features: Enhanced Translation Synchronization, PMU Extensions v3.5, Guest Translation Granule size, Hardware management of access flag/dirty bit state, and Preventing EL0 access to halves of address maps</li>
  <li>ARM: emulation support for Cortex-A35 CPUs</li>
  <li>LoongArch: support for fw_cfg DMA functionality, memory hotplug, and TPM device emulation</li>
  <li>OpenRISC: support for multi-threaded TCG, stability improvements, and new ‘virt’ machine type for CI/device testing.</li>
  <li>RISC-V: ‘virt’ machine support for booting S-mode firmware from pflash, and general device tree improvements</li>
  <li>s390x: support for Message-Security-Assist Extension 5 (RNG via PRNO instruction), SHA-512 via KIMD/KLMD instructions, and enhanced zPCI interpretation support for KVM guests</li>
  <li>x86: TCG performance improvements, including SSE</li>
  <li>x86: TCG support for AVX, AVX2, F16C, FMA3, and VAES instructions</li>
  <li>x86: KVM support for “notify vmexit” mechanism to prevent processor bugs from hanging whole system</li>
  <li>LUKS block device headers are validated more strictly, creating LUKS images is supported on macOS</li>
  <li>Memory backends now support NUMA-awareness when preallocating memory</li>
  <li>and lots more…</li>
</ul>

<p>Thank you to everyone involved!</p>]]></content><author><name></name></author><category term="releases" /><category term="qemu 7.2" /><summary type="html"><![CDATA[We’d like to announce the availability of the QEMU 7.2.0 release. This release contains 1800+ commits from 205 authors.]]></summary></entry><entry><title type="html">Introduction to Zoned Storage Emulation</title><link href="/2022/11/17/zoned-emulation/" rel="alternate" type="text/html" title="Introduction to Zoned Storage Emulation" /><published>2022-11-17T00:00:00+08:00</published><updated>2022-11-17T00:00:00+08:00</updated><id>/2022/11/17/zoned-emulation</id><content type="html" xml:base="/2022/11/17/zoned-emulation/"><![CDATA[<p>This summer I worked on adding Zoned Block Device (ZBD) support to virtio-blk as part of the <a href="https://www.outreachy.org/">Outreachy</a> internship program. QEMU hasn’t directly supported ZBDs before so this article explains how they work and why QEMU needed to be extended.</p>

<h2 id="zoned-block-devices">Zoned block devices</h2>

<p>Zoned block devices (ZBDs) are divided into regions called zones that can only be written sequentially. By only allowing sequential writes, SSD write amplification can be reduced by eliminating the need for a <a href="https://en.wikipedia.org/wiki/Flash_translation_layer">Flash Translation Layer</a>, and potentially lead to higher throughput and increased capacity. Providing a new storage software stack, zoned storage concepts are standardized as <a href="https://zonedstorage.io/docs/introduction/smr#governing-standards">ZBC (SCSI standard), ZAC (ATA standard)</a>, and <a href="https://zonedstorage.io/docs/introduction/zns">ZNS (NVMe)</a>. Meanwhile, the virtio protocol for block devices(virtio-blk) should also be aware of ZBDs instead of taking them as regular block devices. It should be able to pass such devices through to the guest. An overview of necessary work is as follows:</p>

<ol>
  <li>Virtio protocol: <a href="https://lwn.net/Articles/914377/">extend virtio-blk protocol with main zoned storage concept</a>, Dmitry Fomichev</li>
  <li>Linux: <a href="https://www.spinics.net/lists/linux-block/msg91944.html">implement the virtio specification extensions</a>, Dmitry Fomichev</li>
  <li>QEMU: <a href="https://lists.gnu.org/archive/html/qemu-devel/2022-10/msg05195.html">add zoned storage APIs to the block layer</a>, Sam Li</li>
  <li>QEMU: implement zoned storage support in virtio-blk emulation, Sam Li</li>
</ol>

<p>Once the QEMU and Linux patches have been merged it will be possible to expose a virtio-blk ZBD to the guest like this:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">-blockdev</span> node-name<span class="o">=</span>drive0,driver<span class="o">=</span>zoned_host_device,filename<span class="o">=</span>/path/to/zbd,cache.direct<span class="o">=</span>on <span class="se">\</span>
<span class="nt">-device</span> virtio-blk-pci,drive<span class="o">=</span>drive0 <span class="se">\</span>
</code></pre></div></div>

<p>And then we can perform zoned block commands on that device in the guest os.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># blkzone report /dev/vda</span>
start: 0x000000000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x000020000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x000040000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x000060000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x000080000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x0000a0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x0000c0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x0000e0000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 0<span class="o">(</span>nw<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 1<span class="o">(</span>CONVENTIONAL<span class="o">)]</span>
start: 0x000100000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1<span class="o">(</span>em<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 2<span class="o">(</span>SEQ_WRITE_REQUIRED<span class="o">)]</span>
start: 0x000120000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1<span class="o">(</span>em<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 2<span class="o">(</span>SEQ_WRITE_REQUIRED<span class="o">)]</span>
start: 0x000140000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1<span class="o">(</span>em<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 2<span class="o">(</span>SEQ_WRITE_REQUIRED<span class="o">)]</span>
start: 0x000160000, len 0x020000, cap 0x020000, wptr 0x000000 reset:0 non-seq:0, zcond: 1<span class="o">(</span>em<span class="o">)</span> <span class="o">[</span><span class="nb">type</span>: 2<span class="o">(</span>SEQ_WRITE_REQUIRED<span class="o">)]</span>
</code></pre></div></div>

<h2 id="zoned-emulation">Zoned emulation</h2>

<p>Currently, QEMU can support zoned devices by virtio-scsi or PCI device passthrough. It needs to specify the device type it is talking to. Whereas storage controller emulation uses block layer APIs instead of directly accessing disk images. Extending virtio-blk emulation avoids code duplication and simplify the support by hiding the device types under a unified zoned storage interface, simplifying VM deployment for different types of zoned devices. Virtio-blk can also be implemented in hardware. If those devices wish to follow the zoned storage model then the virtio-blk specification needs to natively support zoned storage. With such support, individual NVMe namespaces or anything that is a zoned Linux block device can be exposed to the guest without passing through a full device.</p>

<p>For zoned storage emulation, zoned storage APIs support three zoned models (conventional, host-managed, host-aware) , four zone management commands (Report Zone, Open Zone, Close Zone, Finish Zone), and Append Zone.  The QEMU block layer has a BlockDriverState graph that propagates device information inside block layer. File-posix driver is the lowest level within the graph where zoned storage APIs reside.</p>

<p>After receiving the block driver states, Virtio-blk emulation recognizes zoned devices and sends the zoned feature bit to guest. Then the guest can see the zoned device in the host. When the guest executes zoned operations, virtio-blk driver issues corresponding requests that will be captured by viritio-blk device inside QEMU. Afterwards, virtio-blk device sends the requests to file-posix driver which will perform zoned operations using Linux ioctls.</p>

<p>Unlike zone management operations, Linux doesn’t have a user API to issue zone append requests to zoned devices from user space. With the help of write pointer emulation tracking locations of write pointer of each zone, QEMU block layer can perform append writes by modifying regular writes. Write pointer locks guarantee the execution of requests. Upon failure it must not update the write pointer location which is only got updated when the request is successfully finished.</p>

<p>Problems can always be solved with right mind and right tools. A good approach to avoid pitfalls of programs is test-driven. In the beginning, users like qemu-io commands utility can invoke new block layer APIs. Moving towards to guest, existing tools like blktests, zonefs-tools, and fio are introduced for broader testing. Depending on the size of the zoned device, some tests may take long enough time to finish. Besides, tracing is also a good tool for spotting bugs. QEMU tracking tools and blktrace monitors block layer IO, providing detailed information to analysis.</p>

<h2 id="starting-the-journey-with-open-source">Starting the journey with open source</h2>

<p>As a student interested in computer science, I am enthusiastic about making real applications and fortunate to find the opportunity in this summer. I have a wonderful experience with QEMU where I get chance to work with experienced engineers and meet peers sharing same interests. It is a good starting point for me to continue my search on storage systems and open source projects.</p>

<p>Public communication, reaching out to people and admitting to failures used to be hard for me. Those feelings had faded away as I put more effort to this project over time. For people may having the same trouble as me, it might be useful to focus on the tasks ahead of you instead of worrying about the consequences of rejections from others.</p>

<p>Finally, I would like to thank Stefan Hajnoczi, Damien Le Moal, Dmitry Fomichev, and Hannes Reinecke for mentoring me - they have guided me through this project with patience and expertise, when I hit  obstacles on design or implementations, and introduced a fun and vibrant open source world for me. Also thank QEMU community and Outreachy for organizing this program.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The current status for this project is waiting for virtio specifications extension and Linux driver support patches got accepted. And the up-to-date patch series of zoned device support welcome any new comments.</p>

<p>The next step for zoned storage emulation in QEMU is to enable full zoned emulation through virtio-blk. Adding support on top of a regular file, it allows developers accessing a zoned device environment without real zoned storage hardwares. Furthermore, virtio-scsi may need to add full emulation support to complete the zoned storage picture in QEMU. QEMU NVMe ZNS emulation can also use new block layer APIs to attach real zoned storage if the emulation is used in production in future.</p>]]></content><author><name>Sam Li</name></author><category term="storage" /><category term="gsoc" /><category term="outreachy" /><category term="internships" /><summary type="html"><![CDATA[This summer I worked on adding Zoned Block Device (ZBD) support to virtio-blk as part of the Outreachy internship program. QEMU hasn’t directly supported ZBDs before so this article explains how they work and why QEMU needed to be extended.]]></summary></entry><entry><title type="html">QEMU version 7.1.0 released</title><link href="/2022/08/31/qemu-7-1-0/" rel="alternate" type="text/html" title="QEMU version 7.1.0 released" /><published>2022-08-31T07:00:00+08:00</published><updated>2022-08-31T07:00:00+08:00</updated><id>/2022/08/31/qemu-7-1-0</id><content type="html" xml:base="/2022/08/31/qemu-7-1-0/"><![CDATA[<p>We’d like to announce the availability of the QEMU 7.1.0 release. This release contains 2800+ commits from 238 authors.</p>

<p>You can grab the tarball from our <a href="https://www.qemu.org/download/#source">download page</a>. The full list of changes are available <a href="https://wiki.qemu.org/ChangeLog/7.1">in the Wiki</a>.</p>

<p>Highlights include:</p>

<ul>
  <li>Live migration: support for zero-copy-send on Linux</li>
  <li>QMP: new options for exporting NBD images with dirty bitmaps via ‘block-export-add’ command</li>
  <li>QMP: new ‘query-stats’ and ‘query-stats-schema’ commands for retrieving statistics from various QEMU subsystems</li>
  <li>QEMU guest agent: improved Solaris support, new commands ‘guest-get-diskstats’/’guest-get-cpustats’, ‘guest-get-disks’ now reports NVMe SMART information, and ‘guest-get-fsinfo’ now reports NVMe bus-type</li>
  <li>ARM: emulation support for new machine types: Aspeed AST1030 SoC, Qaulcomm, and fby35 (AST2600 / AST1030)</li>
  <li>ARM: emulation support for Cortex-A76 and Neoverse-N1 CPUs</li>
  <li>ARM: emulation support for Scalable Matrix Extensions, cache speculation control, RAS, and many other CPU extensions</li>
  <li>ARM: ‘virt’ board now supports emulation of GICv4.0</li>
  <li>HPPA: new SeaBIOS v6 firmware with support for PS/2 keyboard in boot menu when running with GTK UI, improved serial port emulation, and additional STI text fonts</li>
  <li>LoongArch: initial support for LoongArch64 architecture, Loongson 3A5000 multiprocessor SoC, and the Loongson 7A1000 host bridge</li>
  <li>MIPS: Nios2 board (-machine 10m50-ghrd) now support Vectored Interrupt Controller, shadow register sets, and improved exception handling</li>
  <li>OpenRISC: ‘or1k-sim’ machine now support 4 16550A UART serial devices instead of 1</li>
  <li>RISC-V: new ISA extensions with support for privileged spec version 1.12.0, software access to MIP SEIP, Sdtrig extension, vector extension improvements, native debug, PMU improvements, and many other features and miscellaneous fixes/improvements</li>
  <li>RISC-V: ‘virt’ board now supports TPM</li>
  <li>RISC-V: ‘OpenTitan’ board now supports Ibex SPI</li>
  <li>s390x: emulation support for s390x Vector-Enhancements Facility 2</li>
  <li>s390x: s390-ccw BIOS now supports booting from drives with non-512 sector sizes</li>
  <li>x86: virtualization support for architectural LBRs</li>
  <li>Xtensa: support for lx106 core and cache testing opcodes</li>
  <li>and lots more…</li>
</ul>

<p>Thank you to everyone involved!</p>]]></content><author><name></name></author><category term="releases" /><category term="qemu 7.1" /><summary type="html"><![CDATA[We’d like to announce the availability of the QEMU 7.1.0 release. This release contains 2800+ commits from 238 authors.]]></summary></entry><entry><title type="html">QEMU version 7.0.0 released</title><link href="/2022/04/20/qemu-7-0-0/" rel="alternate" type="text/html" title="QEMU version 7.0.0 released" /><published>2022-04-20T00:04:00+08:00</published><updated>2022-04-20T00:04:00+08:00</updated><id>/2022/04/20/qemu-7-0-0</id><content type="html" xml:base="/2022/04/20/qemu-7-0-0/"><![CDATA[<p>We’d like to announce the availability of the QEMU 7.0.0 release. This release contains 2500+ commits from 225 authors.</p>

<p>You can grab the tarball from our <a href="https://www.qemu.org/download/#source">download page</a>. The full list of changes are available <a href="https://wiki.qemu.org/ChangeLog/7.0">in the Wiki</a>.</p>

<p>Highlights include:</p>

<ul>
  <li>ACPI: support for logging guest events via ACPI ERST interface</li>
  <li>virtiofs: improved security label support</li>
  <li>block: improved flexibility for fleecing backups, including support for non-qcow2 images</li>
  <li>ARM: ‘virt’ board support for virtio-mem-pci, specifying guest CPU topology, and enabling PAuth when using KVM/hvf</li>
  <li>ARM: ‘xlnx-versal-virt’ board support for PMC SLCR and emulating the OSPI flash memory controller</li>
  <li>ARM: ‘xlnx-zynqmp’ now models the CRF and APU control</li>
  <li>HPPA: support for up to 16 vCPUs, improved graphics driver for HP-UX VDE/CDE environments, setting SCSI boot order, and a number of other new features</li>
  <li>OpenRISC: ‘sim’ board support for up to 4 cores, loading an external initrd image, and automatically generating a device tree for the boot kernel</li>
  <li>PowerPC: ‘pseries’ emulation support for running guests as a nested KVM hypervisor, and new support for spapr-nvdimm device</li>
  <li>PowerPC: ‘powernv’ emulation improvements for XIVE and PHB 3/4, and new support for XIVE2 and PHB5</li>
  <li>RISC-V: support for KVM</li>
  <li>RISC-V: support for ratified 1.0 Vector extension, as well as Zve64f, Zve32f, Zfhmin, Zfh, zfinx, zdinx, and zhinx{min} extensions.</li>
  <li>RISC-V: ‘spike’ machine support for OpenSBI binary loading</li>
  <li>RISC-V: ‘virt’ machine support for 32 cores, and AIA support.</li>
  <li>s390x: support for “Miscellaneous-Instruction-Extensions Facility 3” (a z15 extension)</li>
  <li>x86: Support for Intel AMX</li>
  <li>and lots more…</li>
</ul>

<p>Thank you to everyone involved!</p>]]></content><author><name></name></author><category term="releases" /><category term="qemu 7.0" /><summary type="html"><![CDATA[We’d like to announce the availability of the QEMU 7.0.0 release. This release contains 2500+ commits from 225 authors.]]></summary></entry><entry><title type="html">Apply for a QEMU Google Summer of Code internship</title><link href="/2022/03/07/gsoc-2022/" rel="alternate" type="text/html" title="Apply for a QEMU Google Summer of Code internship" /><published>2022-03-07T21:30:00+08:00</published><updated>2022-03-07T21:30:00+08:00</updated><id>/2022/03/07/gsoc-2022</id><content type="html" xml:base="/2022/03/07/gsoc-2022/"><![CDATA[<p>We have great news to share: QEMU has been accepted as a <a href="https://summerofcode.withgoogle.com/">Google Summer of
Code</a> 2022 organization! Google Summer of
Code is an open source internship program offering paid remote work
opportunities for contributing to open source. The internship runs from June
13th to September 12th.</p>

<p>Now is the chance to get involved in QEMU development! The QEMU community has
put together a list of project ideas
<a href="https://wiki.qemu.org/Google_Summer_of_Code_2022">here</a>.</p>

<p>Google has dropped the requirement that you need to be enrolled in a higher
education course. We’re excited to work with a wider range of contributors this
year! For details on the new eligibility requirements, see
<a href="https://developers.google.com/open-source/gsoc/faq#what_are_the_eligibility_requirements_for_participation">here</a>.</p>

<p>You can <a href="https://wiki.qemu.org/Google_Summer_of_Code_2022#2._Fill_out_the_application_form">submit your
application</a>
from April 4th to 19th.</p>

<p>GSoC interns work together with their mentors, experienced QEMU contributors
who support their interns in their projects. Code developed during the
internship is submitted through the same open source development process that
all QEMU contributions follow. This gives interns experience with contributing
to open source software. Some interns then choose to pursue a career in open
source software after completing their internship.</p>

<p>If you have questions about applying for QEMU GSoC, please email <a href="mailto:stefanha@gmail.com">Stefan
Hajnoczi</a> or ask on the <a href="https://webchat.oftc.net/?channels=qemu-gsoc">#qemu-gsoc IRC
channel</a>.</p>]]></content><author><name></name></author><category term="internships" /><category term="gsoc" /><summary type="html"><![CDATA[We have great news to share: QEMU has been accepted as a Google Summer of Code 2022 organization! Google Summer of Code is an open source internship program offering paid remote work opportunities for contributing to open source. The internship runs from June 13th to September 12th.]]></summary></entry><entry><title type="html">QEMU welcomes Outreachy internship applicants</title><link href="/2022/02/15/outreach-2022/" rel="alternate" type="text/html" title="QEMU welcomes Outreachy internship applicants" /><published>2022-02-15T21:30:00+08:00</published><updated>2022-02-15T21:30:00+08:00</updated><id>/2022/02/15/outreach-2022</id><content type="html" xml:base="/2022/02/15/outreach-2022/"><![CDATA[<p>QEMU is offering open source internships in
<a href="https://www.outreachy.org/">Outreachy’s</a> May-August 2022 round. You can submit
your application until February 25th 2022 if you want to contribute to QEMU in
a remote work internship this summer.</p>

<p>Outreachy internships are extended to people who are subject to systemic bias
and underrepresentation in the technical industry where they are living. For
details on applying, please see the <a href="https://www.outreachy.org/apply/">Outreachy
website</a>. If you are not eligible, don’t
worry, QEMU is also applying to participate in Google Summer of Code again and
we hope to share news about additional internships later this year.</p>

<p>Outreachy interns work together with their mentors, experienced QEMU
contributors who support their interns in their projects. Code developed during
the internship is submitted via the same open source development process that
all QEMU code follows. This gives interns experience with contributing to open
source software. Some interns then choose to pursue a career in open source
software after completing their internship.</p>

<p>Now is the chance to get involved in QEMU development!</p>

<p>If you have questions about applying for QEMU Outreachy, please email <a href="mailto:stefanha@gmail.com">Stefan
Hajnoczi</a> or ask on the <a href="https://webchat.oftc.net/?channels=qemu-gsoc">#qemu-gsoc IRC
channel</a>.</p>]]></content><author><name></name></author><category term="internships" /><category term="outreachy" /><summary type="html"><![CDATA[QEMU is offering open source internships in Outreachy’s May-August 2022 round. You can submit your application until February 25th 2022 if you want to contribute to QEMU in a remote work internship this summer.]]></summary></entry><entry><title type="html">QEMU version 6.2.0 released</title><link href="/2021/12/15/qemu-6-2-0/" rel="alternate" type="text/html" title="QEMU version 6.2.0 released" /><published>2021-12-15T05:32:00+08:00</published><updated>2021-12-15T05:32:00+08:00</updated><id>/2021/12/15/qemu-6-2-0</id><content type="html" xml:base="/2021/12/15/qemu-6-2-0/"><![CDATA[<p>We’d like to announce the availability of the QEMU 6.2.0 release. This release
contains 2300+ commits from 189 authors.</p>

<p>You can grab the tarball from our <a href="https://www.qemu.org/download/#source">download page</a>. The full list of changes are available <a href="https://wiki.qemu.org/ChangeLog/6.2">in the Wiki</a>.</p>

<p>Highlights include:</p>

<ul>
  <li>virtio-mem: guest memory dumps are now fully supported, along with pre-copy/post-copy migration and background guest snapshots</li>
  <li>QMP: support for nw DEVICE_UNPLUG_GUEST_ERROR to detect guest-reported hotplug failures</li>
  <li>TCG: improvements to TCG plugin argument syntax, and multi-core support for cache plugin</li>
  <li>68k: improved support for Apple’s NuBus, including ability to load declaration ROMs, and slot IRQ support</li>
  <li>ARM: macOS hosts with Apple Silicon CPUs now support ‘hvf’ accelerator for AArch64 guests</li>
  <li>ARM: emulation support for Fujitsu A64FX processor model</li>
  <li>ARM: emulation support for kudo-mbc machine type</li>
  <li>ARM: M-profile MVE extension is now supported for Cortex-M55</li>
  <li>ARM: ‘virt’ machine now supports an emulated ITS (Interrupt Translation Service) and supports more than 123 CPUs in emulation mode</li>
  <li>ARM: xlnx-zcu102 and xlnx-versal-virt machines now support BBRAM and eFUSE devices</li>
  <li>PowerPC: improved POWER10 support for the ‘powernv’ machine type</li>
  <li>PowerPC: initial support for POWER10 DD2.0 CPU model</li>
  <li>PowerPC: support for FORM2 PAPR NUMA descriptions for ‘pseries’ machine type</li>
  <li>RISC-V: support for Zb[abcs] instruction set extensions</li>
  <li>RISC-V: support for vhost-user and numa mem options across all boards</li>
  <li>RISC-V: SiFive PWM support</li>
  <li>x86: support for new Snowridge-v4 CPU model</li>
  <li>x86: guest support for Intel SGX</li>
  <li>x86: AMD SEV guests now support measurement of kernel binary when doing direct kernel boot (not using a bootloader)</li>
  <li>and lots more…</li>
</ul>

<p>Thank you to everyone involved!</p>]]></content><author><name></name></author><category term="releases" /><category term="qemu 6.2" /><summary type="html"><![CDATA[We’d like to announce the availability of the QEMU 6.2.0 release. This release contains 2300+ commits from 189 authors.]]></summary></entry><entry><title type="html">QEMU version 6.1.0 released</title><link href="/2021/08/25/qemu-6-1-0/" rel="alternate" type="text/html" title="QEMU version 6.1.0 released" /><published>2021-08-25T04:22:00+08:00</published><updated>2021-08-25T04:22:00+08:00</updated><id>/2021/08/25/qemu-6-1-0</id><content type="html" xml:base="/2021/08/25/qemu-6-1-0/"><![CDATA[<p>We’d like to announce the availability of the QEMU 6.1.0 release. This release
contains 3000+ commits from 221 authors.</p>

<p>You can grab the tarball from our 
<a href="https://www.qemu.org/download/#source">download page</a>.
The full list of changes are available
<a href="https://wiki.qemu.org/ChangeLog/6.1">in the Wiki</a>.</p>

<p>Highlights include:</p>

<ul>
  <li>block: support for changing block node options after creation via ‘blockdev-reopen’ QMP command</li>
  <li>Crypto: more performant backend recommendations and improved documentation</li>
  <li>I2C: emulation support for I2C muxes (pca9546, pca9548) and PMBus</li>
  <li>TCG Plugins: now enabled by default, with new execlog and <a href="https://www.qemu.org/2021/08/19/tcg-cache-modelling-plugin/">cache modelling plugins</a>.</li>
  <li>ARM: new board support for Aspeed (rainier-bmc, quanta-q7l1), npcm7xx (quanta-gbs-bmc), and Cortex-M3 (stm32vldiscovery) based machines</li>
  <li>ARM: Aspeed support of Hash and Crypto Engine</li>
  <li>ARM: emulation support for SVE2 (including bfloat16), integer matrix multiply accumulate operations, TLB invalidate in Outer Shareable domain, TLB range invalidate, and more.</li>
  <li>PowerPC: pseries: support for detecting hotplug failures in newer guests</li>
  <li>PowerPC: pseries: increased maximum CPU count</li>
  <li>PowerPC: pseries: emulation support for some POWER10 prefixed instructions</li>
  <li>PowerPC: new board support for Genesi/bPlan Pegasos II (pegasos2)</li>
  <li>RISC-V: updates to OpenTitan platform support, including OpenTitan timer</li>
  <li>RISC-V: support for virtio-vga</li>
  <li>RISC-V: documentation improvements and general code cleanups/fixes</li>
  <li>s390: emulation support for the vector-enhancements facility</li>
  <li>s390: support for gen16 CPU models</li>
  <li>x86: new Intel CPU model versions with support for XSAVES instruction</li>
  <li>x86: added ACPI based PCI hotplug support for Q35 machine (now the default)</li>
  <li>x86: improvements to emulation of AMD virtualization extensions</li>
  <li>and lots more…</li>
</ul>

<p>Thank you to everyone involved!</p>]]></content><author><name></name></author><category term="releases" /><category term="qemu 6.1" /><summary type="html"><![CDATA[We’d like to announce the availability of the QEMU 6.1.0 release. This release contains 3000+ commits from 221 authors.]]></summary></entry><entry><title type="html">Presenting guest images as raw image files with FUSE</title><link href="/2021/08/22/fuse-blkexport/" rel="alternate" type="text/html" title="Presenting guest images as raw image files with FUSE" /><published>2021-08-22T20:00:00+08:00</published><updated>2021-09-06T18:30:00+08:00</updated><id>/2021/08/22/fuse-blkexport</id><content type="html" xml:base="/2021/08/22/fuse-blkexport/"><![CDATA[<p>Sometimes, there is a VM disk image whose contents you want to manipulate
without booting the VM.  One way of doing this is to use
<a href="https://libguestfs.org">libguestfs</a>, which can boot a minimal Linux VM to
provide the host with secure access to the disk’s contents.  For example,
<a href="https://libguestfs.org/guestmount.1.html"><em>guestmount</em></a> allows you to mount a
guest filesystem on the host, without requiring root rights.</p>

<p>However, maybe you cannot or do not want to use libguestfs, e.g. because you do
not have KVM available in your environment, and so it becomes too slow; or
because you do not want to go through a guest OS, but want to access the raw
image data directly on the host, with minimal overhead.</p>

<p><strong>Note</strong>: Guest images can generally be arbitrarily modified by VM guests.  If
you have an image to which an untrusted guest had write access at some point,
you must treat any data and metadata on this image as potentially having been
modified in a malicious manner.  Parsing anything must be done carefully and
with caution.  Note that many existing tools are not careful in this regard, for
example, filesystem drivers generally deliberately do not have protection
against maliciously corrupted filesystems.  This is why in contrast accessing an
image through libguestfs is considered secure, because the actual access happens
in a libvirt-managed VM guest.</p>

<p>From this point, we assume you are aware of the security caveats and still want
to access and manipulate image data on the host.</p>

<p>Now, unless your image is already in raw format, you will be faced with the
problem of getting it into raw format.  The tools that you might want to use for
image manipulation generally only work on raw images (because that is how block
device files appear), like:</p>
<ul>
  <li><em>dd</em> to just copy data to and from given offsets,</li>
  <li><em>parted</em> to manipulate the partition table,</li>
  <li><em>kpartx</em> to present all partitions as block devices,</li>
  <li><em>mount</em> to access filesystems’ contents.</li>
</ul>

<p>So if you want to use such tools on image files e.g. in QEMU’s qcow2 format, you
will need to translate them into raw images first, for example by:</p>
<ul>
  <li>Exporting the image file with <code class="highlighter-rouge">qemu-nbd -c</code> as an NBD block device file,</li>
  <li>Converting between image formats using <code class="highlighter-rouge">qemu-img convert</code>,</li>
  <li>Accessing the image from a guest, where it appears as a normal block device.</li>
</ul>

<p>Unfortunately, none of these methods is perfect: <code class="highlighter-rouge">qemu-nbd -c</code> generally
requires root rights; converting to a temporary raw copy requires additional
disk space and the conversion process takes time; and accessing the image from a
guest is basically what libguestfs does (i.e., if that is what you want, then
you should probably use libguestfs).</p>

<p>As of QEMU 6.0, there is another method, namely FUSE block exports.
Conceptually, these are rather similar to using <code class="highlighter-rouge">qemu-nbd -c</code>, but they do not
require root rights.</p>

<p><strong>Note</strong>: FUSE block exports are a feature that can be enabled or disabled
during the build process with <code class="highlighter-rouge">--enable-fuse</code> or <code class="highlighter-rouge">--disable-fuse</code>, respectively;
omitting either configure option will enable the feature if and only if libfuse3
is present.  It is possible that the QEMU build you are using does not have FUSE
block export support, because it was not compiled in.</p>

<p>FUSE (<em>Filesystem in Userspace</em>) is a technology to let userspace processes
provide filesystem drivers.  For example, <em>sshfs</em> is a program that allows
mounting remote directories from a machine accessible via SSH.</p>

<p>QEMU can use FUSE to make a virtual block device appear as a normal file on the
host, so that tools like <em>kpartx</em> can interact with it regardless of the image
format, like in the following example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img create -f raw foo.img 20G
Formatting 'foo.img', fmt=raw size=21474836480

$ parted -s foo.img \
    'mklabel msdos' \
    'mkpart primary ext4 2048s 100%'

$ qemu-img convert -p -f raw -O qcow2 foo.img foo.qcow2 &amp;&amp; rm foo.img
    (100.00/100%)

$ file foo.qcow2
foo.qcow2: QEMU QCOW2 Image (v3), 21474836480 bytes

$ sudo kpartx -l foo.qcow2

$ qemu-storage-daemon \
    --blockdev node-name=prot-node,driver=file,filename=foo.qcow2 \
    --blockdev node-name=fmt-node,driver=qcow2,file=prot-node \
    --export \
    type=fuse,id=exp0,node-name=fmt-node,mountpoint=foo.qcow2,writable=on \
    &amp;
[1] 200495

$ file foo.qcow2
foo.qcow2: DOS/MBR boot sector; partition 1 : ID=0x83, start-CHS (0x10,0,1),
end-CHS (0x3ff,3,32), startsector 2048, 41940992 sectors

$ sudo kpartx -av foo.qcow2
add map loop0p1 (254:0): 0 41940992 linear 7:0 2048
</code></pre></div></div>

<p>In this example, we create a partition on a newly created raw image.  We then
convert this raw image to qcow2 and discard the original.  Because a tool like
<em>kpartx</em> cannot parse the qcow2 format, it reports no partitions to be present
in <code class="highlighter-rouge">foo.qcow2</code>.</p>

<p>Using the QEMU storage daemon, we then create a FUSE export for the image that
apparently turns it into a raw image, which makes the content and thus the
partitions visible to <em>file</em> and <em>kpartx</em>.  Now, we can use <em>kpartx</em> to access
the partition in <code class="highlighter-rouge">foo.qcow2</code> under <code class="highlighter-rouge">/dev/mapper/loop0p1</code>.</p>

<p>So how does this work?  How can the QEMU storage daemon make a qcow2 image
appear as a raw image?</p>

<h2 id="file-mounts">File mounts</h2>

<p>To transparently translate a file into a different format, like we did above, we
make use of two little-known facts about filesystems and the VFS on Linux.  The
first one of these we can explain immediately, for the second one we will need
some more information about how FUSE exports work, so that secret will be lifted
later (down in the “Mounting an image on itself” section).</p>

<p>Here is the first secret: Filesystems do not need to have a root directory.
They only need a root node.  A regular file is a node, so a filesystem that only
consists of a single regular file is perfectly valid.</p>

<p>Note that this is not about filesystems with just a single file in their root
directory, but about filesystems that really <em>do not have</em> a root directory.</p>

<p>Conceptually, every filesystem is a tree, and mounting works by replacing one
subtree of the global VFS tree by the mounted filesystem’s tree.  Normally, a
filesystem’s root node is a directory, like in the following example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-root-directory.svg" alt="Regular filesystem: Root directory is mounted to a directory mount point" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 1: Mounting a regular filesystem with a directory as its root node</em></td>
    </tr>
  </tbody>
</table>

<p>Here, the directory <code class="highlighter-rouge">/foo</code> and its content (the files <code class="highlighter-rouge">/foo/a</code> and <code class="highlighter-rouge">/foo/b</code>) are
shadowed by the new filesystem (showing <code class="highlighter-rouge">/foo/x</code> and <code class="highlighter-rouge">/foo/y</code>).</p>

<p>Note that a filesystem’s root node generally has no name.  After mounting, the
filesystem’s root directory’s name is determined by the original name of the
mount point.  (“/” is not a name.  It specifically is a directory without a
name.)</p>

<p>Because a tree does not need to have multiple nodes but may consist of just a
single leaf, a filesystem with a file for its root node works just as well,
though:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-root-file.svg" alt="Mounting a file root node to a regular file mount point" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 2: Mounting a filesystem with a regular (unnamed) file as its root node</em></td>
    </tr>
  </tbody>
</table>

<p>Here, FS B only consists of a single node, a regular file with no name.  (As
above, a filesystem’s root node is generally unnamed.) Consequently, the mount
point for it must also be a regular file (<code class="highlighter-rouge">/foo/a</code> in our example), and just
like before, the content of <code class="highlighter-rouge">/foo/a</code> is shadowed, and when opening it, one will
instead see the contents of FS B’s unnamed root node.</p>

<h2 id="qemu-block-exports">QEMU block exports</h2>

<p>Before we can see what FUSE exports are and how they work, we should explore
QEMU block exports in general.</p>

<p>QEMU allows exporting block nodes via various protocols (as of 6.0: NBD,
vhost-user, FUSE).  A block node is an element of QEMU’s block graph (see e.g.
<a href="http://events17.linuxfoundation.org/sites/events/files/slides/talk\_11.pdf">Managing the New Block Layer</a>,
a talk given at KVM Forum 2017), which can for example be attached to guest
devices.  Here is a very simple example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-block-graph-a.svg" alt="Block graph: image file &lt;-&gt; file node (label: prot-node) &lt;-&gt; qcow2 node (label: fmt-node) &lt;-&gt; virtio-blk guest device" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 3: A simple block graph for attaching a qcow2 image to a virtio-blk guest device</em></td>
    </tr>
  </tbody>
</table>

<p>This is the simplest example for a block graph that connects a <em>virtio-blk</em>
guest device to a qcow2 image file.  The <em>file</em> block driver, instanced in the
form of a block node named <em>prot-node</em>, accesses the actual file and provides
the node above it access to the raw content.  This node above, named <em>fmt-node</em>,
is handled by the <em>qcow2</em> block driver, which is capable of interpreting the
qcow2 format.  Parents of this node will therefore see the actual content of the
virtual disk that is represented by the qcow2 image.  There is only one parent
here, which is the <em>virtio-blk</em> guest device, which will thus see the virtual
disk.</p>

<p>The command line to achieve the above could look something like this:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-system-x86_64 \
    -blockdev node-name=prot-node,driver=file,filename=$image_path \
    -blockdev node-name=fmt-node,driver=qcow2,file=prot-node \
    -device virtio-blk,drive=fmt-node,share-rw=on
</code></pre></div></div>

<p>Besides attaching guest devices to block nodes, you can also export them for
users outside of qemu, for example via NBD.  Say you have a QMP channel open for
the QEMU instance above, then you could do this:</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"execute"</span><span class="p">:</span><span class="w"> </span><span class="s2">"nbd-server-start"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"arguments"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"addr"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"inet"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"data"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
                </span><span class="nl">"host"</span><span class="p">:</span><span class="w"> </span><span class="s2">"localhost"</span><span class="p">,</span><span class="w">
                </span><span class="nl">"port"</span><span class="p">:</span><span class="w"> </span><span class="s2">"10809"</span><span class="w">
            </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="p">{</span><span class="w">
    </span><span class="nl">"execute"</span><span class="p">:</span><span class="w"> </span><span class="s2">"block-export-add"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"arguments"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"nbd"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"exp0"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"node-name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"fmt-node"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"guest-disk"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"writable"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This opens an NBD server on <code class="highlighter-rouge">localhost:10809</code>, which exports <em>fmt-node</em> (under
the NBD export name <em>guest-disk</em>).  The block graph looks as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-block-graph-b.svg" alt="Same block graph as fig. 3, but with an NBD server attached to fmt-node" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 4: Block graph extended by an NBD server</em></td>
    </tr>
  </tbody>
</table>

<p>NBD clients connecting to this server will see the raw disk as seen by the
guest – we have <em>exported</em> the guest disk:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img info nbd://localhost/guest-disk
image: nbd://localhost:10809/guest-disk
file format: raw
virtual size: 20 GiB (21474836480 bytes)
disk size: unavailable
</code></pre></div></div>

<h3 id="qemu-storage-daemon">QEMU storage daemon</h3>

<p>If you are not running a guest, and so do not need guest devices, but all you
want is to use the QEMU block layer (for example to interpret the qcow2 format)
and export nodes from the block graph, then you can use the more lightweight
QEMU storage daemon instead of a full-blown QEMU process:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-storage-daemon \
    --blockdev node-name=prot-node,driver=file,filename=$image_path \
    --blockdev node-name=fmt-node,driver=qcow2,file=prot-node \
    --nbd-server addr.type=inet,addr.host=localhost,addr.port=10809 \
    --export \
    type=nbd,id=exp0,node-name=fmt-node,name=guest-disk,writable=on
</code></pre></div></div>

<p>Which creates the following block graph:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-block-graph-c.svg" alt="Block graph: image file &lt;-&gt; file node (label: prot-node) &lt;-&gt; qcow2 node (label: fmt-node) &lt;-&gt; NBD server" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 5: Exporting a qcow2 image over NBD</em></td>
    </tr>
  </tbody>
</table>

<h2 id="fuse-block-exports">FUSE block exports</h2>

<p>Besides NBD exports, QEMU also supports vhost-user and FUSE exports.  FUSE block
exports make QEMU become a FUSE driver that provides a filesystem that consists
of only a single node, namely a regular file that has the raw contents of the
exported block node.  QEMU will automatically mount this filesystem on a given
existing regular file (which acts as the mount point, as described in the
“File mounts” section).</p>

<p>Thus, FUSE exports can be used like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ touch mount-point

$ qemu-storage-daemon \
  --blockdev node-name=prot-node,driver=file,filename=$image_path \
  --blockdev node-name=fmt-node,driver=qcow2,file=prot-node \
  --export \
  type=fuse,id=exp0,node-name=fmt-node,mountpoint=mount-point,writable=on
</code></pre></div></div>

<p>The mount point now appears as the raw VM disk that is stored in the qcow2
image:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img info mount-point
image: mount-point
file format: raw
virtual size: 20 GiB (21474836480 bytes)
disk size: 196 KiB
</code></pre></div></div>

<p>And <em>mount</em> tells us that this is indeed its own filesystem:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ mount | grep mount-point
/dev/fuse on /tmp/mount-point type fuse (rw,nosuid,nodev,relatime,user_id=1000,
group_id=100,default_permissions,allow_other,max_read=67108864)
</code></pre></div></div>

<p>The block graph looks like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-block-graph-d.svg" alt="Block graph: image file &lt;-&gt; file node (label: prot-node) &lt;-&gt; qcow2 node (label: fmt-node) &lt;-&gt; FUSE server &lt;-&gt; exported file" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 6: Exporting a qcow2 image over FUSE</em></td>
    </tr>
  </tbody>
</table>

<p>Closing the storage daemon (e.g. with Ctrl-C) automatically unmounts the export,
turning the mount point back into an empty normal file:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ mount | grep -c mount-point
0

$ qemu-img info mount-point
image: mount-point
file format: raw
virtual size: 0 B (0 bytes)
disk size: 0 B
</code></pre></div></div>

<h2 id="mounting-an-image-on-itself">Mounting an image on itself</h2>

<p>So far, we have seen what FUSE exports are, how they work, and how they can be
used.  However, in the very first example in this blog post, we did not export
the raw image on some empty regular file that just serves as a mount point – no,
we turned the original qcow2 image itself into a raw image.</p>

<p>How does that work?</p>

<h3 id="what-happens-to-the-old-tree-under-a-mount-point">What happens to the old tree under a mount point?</h3>

<p>Mounting a filesystem only shadows the mount point’s original content, it does
not remove it.  The original content can no longer be looked up via its
(absolute) path, but it is still there, much like a file that has been unlinked
but is still open in some process.  Here is an example:</p>

<p>First, create some file in some directory, and have some process keep it open:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ mkdir foo

$ echo 'Is anyone there?' &gt; foo/bar

$ irb
irb(main):001:0&gt; f = File.open('foo/bar', 'r+')
=&gt; #&lt;File:foo/bar&gt;
irb(main):002:0&gt; ^Z
[1]  + 35494 suspended  irb
</code></pre></div></div>

<p>Next, mount something on the directory:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo mount -t tmpfs tmpfs foo
</code></pre></div></div>

<p>The file cannot be found anymore (because <em>foo</em>’s content is shadowed by the
mounted filesystem), but the process who kept it open can still read from it,
and write to it:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls foo

$ cat foo/bar
cat: foo/bar: No such file or directory

$ fg
f.read
irb(main):002:0&gt; f.read
=&gt; "Is anyone there?\n"
irb(main):003:0&gt; f.puts('Hello from the shadows!')
=&gt; nil
irb(main):004:0&gt; exit

$ ls foo

$ cat foo/bar
cat: foo/bar: No such file or directory
</code></pre></div></div>

<p>Unmounting the filesystem lets us see our file again, with its updated content:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo umount foo

$ ls foo
bar

$ cat foo/bar
Is anyone there?
Hello from the shadows!
</code></pre></div></div>

<h3 id="letting-a-fuse-export-shadow-its-image-file">Letting a FUSE export shadow its image file</h3>

<p>The same principle applies to file mounts: The original inode is shadowed (along
with its content), but it is still there for any process that opened it before
the mount occurred.  Because QEMU (or the storage daemon) opens the image file
before mounting the FUSE export, you can therefore specify an image’s path as
the mount point for its corresponding export:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img create -f qcow2 foo.qcow2 20G
Formatting 'foo.qcow2', fmt=qcow2 cluster_size=65536 extended_l2=off
 compression_type=zlib size=21474836480 lazy_refcounts=off refcount_bits=16

$ qemu-img info foo.qcow2
image: foo.qcow2
file format: qcow2
virtual size: 20 GiB (21474836480 bytes)
disk size: 196 KiB
cluster_size: 65536
Format specific information:
    compat: 1.1
    compression type: zlib
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
    extended l2: false

$ qemu-storage-daemon --blockdev \
   node-name=node0,driver=qcow2,file.driver=file,file.filename=foo.qcow2 \
   --export \
   type=fuse,id=node0-export,node-name=node0,mountpoint=foo.qcow2,writable=on &amp;
[1] 40843

$ qemu-img info foo.qcow2
image: foo.qcow2
file format: raw
virtual size: 20 GiB (21474836480 bytes)
disk size: 196 KiB

$ kill %1
[1]  + 40843 done       qemu-storage-daemon --blockdev  --export
</code></pre></div></div>

<p>In graph form, that looks like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/screenshots/2021-08-18-block-graph-e.svg" alt="Two graphs: First, foo.qcow2 is opened by QEMU; second, a FUSE server exports the raw disk under foo.qcow2, thus shadowing the original foo.qcow2" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Fig. 6: Exporting a qcow2 image via FUSE on its own path</em></td>
    </tr>
  </tbody>
</table>

<p>QEMU (or the storage daemon in this case) keeps the original (qcow2) file open,
and so it keeps access to it, even after the mount.  However, any other process
that opens the image by name (i.e. <code class="highlighter-rouge">open("foo.qcow2")</code>) will open the raw disk
image exported by QEMU.  Therefore, it looks like the qcow2 image is in raw
format now.</p>

<h3 id="qemu-fuse-disk-exportpy"><em>qemu-fuse-disk-export.py</em></h3>

<p>Because the QEMU storage daemon command line tends to become kind of long, I’ve
written a script to facilitate the process:
<a href="https://gitlab.com/hreitz/qemu-scripts/-/blob/main/qemu-fuse-disk-export.py"><em>qemu-fuse-disk-export.py</em></a>
(<a href="https://gitlab.com/hreitz/qemu-scripts/-/raw/main/qemu-fuse-disk-export.py?inline=false">direct download link</a>).
This script automatically detects the image format, and its <code class="highlighter-rouge">--daemonize</code> option
allows safe use in scripts, where it is important that the process blocks until
the export is fully set up.</p>

<p>Using <em>qemu-fuse-disk-export.py</em>, the above example looks like this:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img info foo.qcow2 | grep 'file format'
file format: qcow2

$ qemu-fuse-disk-export.py foo.qcow2 &amp;
[1] 13339
All exports set up, ^C to revert

$ qemu-img info foo.qcow2 | grep 'file format'
file format: raw

$ kill -SIGINT %1
[1]  + 13339 done       qemu-fuse-disk-export.py foo.qcow2

$ qemu-img info foo.qcow2 | grep 'file format'
file format: qcow2
</code></pre></div></div>

<p>Or, with <code class="highlighter-rouge">--daemonize</code>/<code class="highlighter-rouge">-d</code>:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-img info foo.qcow2 | grep 'file format'
file format: qcow2

$ qemu-fuse-disk-export.py -dp qfde.pid foo.qcow2

$ qemu-img info foo.qcow2 | grep 'file format'
file format: raw

$ kill -SIGINT $(cat qfde.pid)

$ qemu-img info foo.qcow2 | grep 'file format'
file format: qcow2
</code></pre></div></div>

<h2 id="bringing-it-all-together">Bringing it all together</h2>

<p>Now we know how to make disk images in any format understood by QEMU appear as
raw images.  We can thus run any application on them that works with such raw
disk images:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-fuse-disk-export.py \
    -dp qfde.pid \
    Arch-Linux-x86_64-basic-20210711.28787.qcow2

$ parted Arch-Linux-x86_64-basic-20210711.28787.qcow2 p
WARNING: You are not superuser.  Watch out for permissions.
Model:  (file)
Disk /tmp/Arch-Linux-x86_64-basic-20210711.28787.qcow2: 42.9GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name  Flags
 1      1049kB  2097kB  1049kB                     bios_grub
 2      2097kB  42.9GB  42.9GB  btrfs

$ sudo kpartx -av Arch-Linux-x86_64-basic-20210711.28787.qcow2
add map loop0p1 (254:0): 0 2048 linear 7:0 2048
add map loop0p2 (254:1): 0 83881951 linear 7:0 4096

$ sudo mount /dev/mapper/loop0p2 /mnt/tmp

$ ls /mnt/tmp
bin   boot  dev  etc  home  lib  lib64  mnt  opt  proc  root  run  sbin  srv
swap  sys   tmp  usr  var

$ echo 'Hello, qcow2 image!' &gt; /mnt/tmp/home/arch/hello

$ sudo umount /mnt/tmp

$ sudo kpartx -d Arch-Linux-x86_64-basic-20210711.28787.qcow2
loop deleted : /dev/loop0

$ kill -SIGINT $(cat qfde.pid)
</code></pre></div></div>

<p>And launching the image, in the guest we see:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[arch@archlinux ~] cat hello
Hello, qcow2 image!
</code></pre></div></div>

<h2 id="a-note-on-allow_other">A note on <code class="highlighter-rouge">allow_other</code></h2>

<p>In the example presented in the above section, we access the exported image with
a different user than the one who exported it (to be specific, we export it as a
normal user, and then access it as root).  This does not work prior to QEMU 6.1:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ qemu-fuse-disk-export.py -dp qfde.pid foo.qcow2

$ sudo stat foo.qcow2
stat: cannot statx 'foo.qcow2': Permission denied
</code></pre></div></div>

<p>QEMU 6.1 has introduced support for FUSE’s <code class="highlighter-rouge">allow_other</code> mount option.  Without
that option, only the user who exported the image has access to it.  By default,
if the system allows for non-root users to add <code class="highlighter-rouge">allow_other</code> to FUSE mount
options, QEMU will add it, and otherwise omit it.  It does so by simply
attempting to mount the export with <code class="highlighter-rouge">allow_other</code> first, and if that fails, it
will try again without.  (You can also force the behavior with the
<code class="highlighter-rouge">allow_other=(on|off|auto)</code> export parameter.)</p>

<p>Non-root users can pass <code class="highlighter-rouge">allow_other</code> if and only if <code class="highlighter-rouge">/etc/fuse.conf</code> contains
the <code class="highlighter-rouge">user_allow_other</code> option.</p>

<h2 id="conclusion">Conclusion</h2>

<p>As shown in this blog post, FUSE block exports are a relatively simple way to
access images in any format understood by QEMU as if they were raw images.
Any tool that can manipulate raw disk images can thus manipulate images in any
format, simply by having the QEMU storage daemon provide a translation layer.
By mounting the FUSE export on the original image path, this translation layer
will effectively be invisible, and the original image will look like it is in
raw format, so it can directly be accessed by those tools.</p>

<p>The current main disadvantage of FUSE exports is that they offer relatively bad
performance.  That should be fine as long as your use case is just light
manipulation of some VM images, like manually modifying some files on them.
However, we did not yet really try to optimize performance, so if more serious
use cases appear that would require better performance, we can try.</p>]]></content><author><name>Hanna Reitz</name></author><category term="storage" /><category term="features" /><category term="tutorials" /><summary type="html"><![CDATA[Sometimes, there is a VM disk image whose contents you want to manipulate without booting the VM. One way of doing this is to use libguestfs, which can boot a minimal Linux VM to provide the host with secure access to the disk’s contents. For example, guestmount allows you to mount a guest filesystem on the host, without requiring root rights.]]></summary></entry><entry><title type="html">Cache Modelling TCG Plugin</title><link href="/2021/08/19/tcg-cache-modelling-plugin/" rel="alternate" type="text/html" title="Cache Modelling TCG Plugin" /><published>2021-08-19T16:00:00+08:00</published><updated>2021-08-19T16:00:00+08:00</updated><id>/2021/08/19/tcg-cache-modelling-plugin</id><content type="html" xml:base="/2021/08/19/tcg-cache-modelling-plugin/"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/CPU_cache">Caches</a> are a key way that enables
modern CPUs to keep running at full speed by avoiding the need to fetch data
and instructions from the comparatively slow system memory. As a result
understanding cache behaviour is a key part of performance optimisation.</p>

<p>TCG plugins provide means to instrument generated code for both user-mode and
full system emulation. This includes the ability to intercept every memory
access and instruction execution. This post introduces a new TCG plugin that’s
used to simulate configurable L1 separate instruction cache and data cache.</p>

<p>While different microarchitectures often have different approaches at the very
low level, the core concepts of caching are universal. As QEMU is not a
microarchitectural emulator we model an ideal caching system with a few simple
parameters. By doing so, we can adequately simulate the behaviour of L1 private
(per-core) caches.</p>

<h2 id="overview">Overview</h2>

<p>The plugin simulates how L1 user-configured caches would behave when given a
working set defined by a program in user-mode, or system-wide working set.
Subsequently, it logs performance statistics along with the most N
cache-thrashing instructions.</p>

<h3 id="configurability">Configurability</h3>

<p>The plugin is configurable in terms of:</p>

<ul>
  <li>icache size parameters: <code class="highlighter-rouge">icachesize</code>, <code class="highlighter-rouge">iblksize</code>, <code class="highlighter-rouge">iassoc</code>, All of which take
a numeric value</li>
  <li>dcache size parameters: <code class="highlighter-rouge">dcachesize</code>, <code class="highlighter-rouge">dblksize</code>, <code class="highlighter-rouge">dassoc</code>. All of which take
a numeric value</li>
  <li>Eviction policy: <code class="highlighter-rouge">evict=lru|rand|fifo</code></li>
  <li>How many top-most thrashing instructions to log: <code class="highlighter-rouge">limit=TOP_N</code></li>
  <li>How many core caches to keep track of: <code class="highlighter-rouge">cores=N_CORES</code></li>
</ul>

<h3 id="multicore-caching">Multicore caching</h3>

<p>Multicore caching is achieved by having independent L1 caches for each available
core.</p>

<p>In <strong>full-system emulation</strong>, the number of available vCPUs is known to the
plugin at plugin installation time, so separate caches are maintained for those.</p>

<p>In <strong>user-space emulation</strong>, the index of the vCPU initiating memory access
monotonically increases and is limited with however much the kernel allows
creating. The approach used is that we allocate a static number of caches, and
fit all memory accesses into those cores. This approximation is sufficiently
similar to real systems since having more threads than cores will result in
interleaving those threads between the available cores so they might thrash each
other anyway.</p>

<h2 id="design-and-implementation">Design and implementation</h2>

<h3 id="general-structure">General structure</h3>

<p>A generic cache data structure, <code class="highlighter-rouge">Cache</code>, is used to model either an icache or
dcache. For each known core, the plugin maintains an icache and a dcache. On a
memory access coming from a core, the corresponding cache is interrogated.</p>

<p>Each cache has a number of cache sets that are used to store the actual cached
locations alongside metadata that backs eviction algorithms. The structure of a
cache with <code class="highlighter-rouge">n</code> sets, and <code class="highlighter-rouge">m</code> blocks per sets is summarized in the following
figure:</p>

<p><img src="/screenshots/2021-06-17-cache-structure.png" alt="cache structure" /></p>

<h3 id="eviction-algorithms">Eviction algorithms</h3>

<p>The plugin supports three eviction algorithms:</p>

<ul>
  <li>Random eviction</li>
  <li>Least recently used (LRU)</li>
  <li>FIFO eviction</li>
</ul>

<h4 id="random-eviction">Random eviction</h4>

<p>On a cache miss that requires eviction, a randomly chosen block is evicted to
make room for the newly-fetched block.</p>

<p>Using random eviction effectively requires no metadata for each set.</p>

<h4 id="least-recently-used-lru">Least recently used (LRU)</h4>

<p>For each set, a generation number is maintained that is incremented on each
memory access and. The current generation number is assigned to the block
currently being accessed. On a cache miss, the block with the least generation
number is evicted.</p>

<h4 id="fifo-eviction">FIFO eviction</h4>

<p>A FIFO queue instance is maintained for each set. On a cache miss, the evicted
block is the first-in block, and the newly-fetched block is enqueued as the
last-in block.</p>

<h2 id="usage">Usage</h2>

<p>Now a simple example usage of the plugin is demonstrated by running a program
that does matrix multiplication, and how the plugin helps identify code that
thrashes the cache.</p>

<p>A program, <code class="highlighter-rouge">test_mm</code> uses the following function to carry out matrix
multiplication:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void mm(int n, int m1[n][n], int m2[n][n], int res[n][n])
{
    for (int i = 0; i &lt; n; i++) {
        for (int j = 0; j &lt; n; j++) {
            int sum = 0;
            for (int k = 0; k &lt; n; k++) {
                int op1 = m1[i][k];
                int op2 = m2[k][j];
                sum += op1 * op2;
            }
            res[i][j] = sum;
        }
    }
}
</code></pre></div></div>

<p>Running <code class="highlighter-rouge">mm_test</code> inside QEMU using the following command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./qemu-x86_64 $(QEMU_ARGS) \
  -plugin ./contrib/plugins/libcache.so,dcachesize=8192,dassoc=4,dblksize=64,\
      icachesize=8192,iassoc=4,iblksize=64 \
  -d plugin \
  -D matmul.log \
  ./mm_test
</code></pre></div></div>

<p>The preceding command will run QEMU and attach the plugin with the following
configuration:</p>

<ul>
  <li>dcache: cache size = 8KBs, associativity = 4, block size = 64B.</li>
  <li>icache: cache size = 8KBs, associativity = 4, block size = 64B.</li>
  <li>Default eviction policy is LRU (used for both caches).</li>
  <li>Default number of cores is 1.</li>
</ul>

<p>The following data is logged in <code class="highlighter-rouge">matmul.log</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>core #, data accesses, data misses, dmiss rate, insn accesses, insn misses, imiss rate
0       4908419        274545          5.5933%  8002457        1005            0.0126%

address, data misses, instruction
0x4000001244 (mm), 262138, movl (%rdi, %rsi, 4), %esi
0x400000121c (mm), 5258, movl (%rdi, %rsi, 4), %esi
0x4000001286 (mm), 4096, movl %edi, (%r8, %rsi, 4)
0x400000199c (main), 257, movl %edx, (%rax, %rcx, 4)

...
</code></pre></div></div>

<p>We can observe two things from the logs:</p>

<ul>
  <li>The most cache-thrashing instructions belong to a symbol called <code class="highlighter-rouge">mm</code>, which
  happens to be the matrix multiplication function.</li>
  <li>Some array-indexing instructions are generating the greatest share of data
  misses.</li>
</ul>

<p><code class="highlighter-rouge">test_mm</code> does a bunch of other operations other than matrix multiplication.
However, Using the plugin data, we can narrow our investigation space to <code class="highlighter-rouge">mm</code>,
which happens to be generating about 98% of the overall number of misses.</p>

<p>Now we need to find out why is the instruction at address <code class="highlighter-rouge">0x4000001224</code>
thrashing the cache. Looking at the disassembly of the program, using
<code class="highlighter-rouge">objdump -Sl test_mm</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/test_mm.c:11 (discriminator 3)
                int op2 = m2[k][j];  &lt;- The line of code we're interested in
    1202:   8b 75 c0               mov    -0x40(%rbp),%esi
    1205:   48 63 fe               movslq %esi,%rdi
    1208:   48 63 f2               movslq %edx,%rsi
    120b:   48 0f af f7            imul   %rdi,%rsi
    120f:   48 8d 3c b5 00 00 00   lea    0x0(,%rsi,4),%rdi
    1216:   00
    1217:   48 8b 75 a8            mov    -0x58(%rbp),%rsi
    121b:   48 01 f7               add    %rsi,%rdi
    121e:   8b 75 c8               mov    -0x38(%rbp),%esi
    1221:   48 63 f6               movslq %esi,%rsi
    1224:   8b 34 b7               mov    (%rdi,%rsi,4),%esi
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
    1227:   89 75 d4               mov    %esi,-0x2c(%rbp)
</code></pre></div></div>

<p>It can be seen that the most problematic instruction is associated with loading
<code class="highlighter-rouge">m2[k][j]</code>. This happens because we’re traversing <code class="highlighter-rouge">m2</code> in a column-wise order.
So if the matrix <code class="highlighter-rouge">m2</code> is larger than the data cache, we end up with fetching
blocks that we only use one integer from and not use again before getting
evicted.</p>

<p>A simple solution to this problem is to <a href="https://en.wikipedia.org/wiki/Transpose">transpose</a>
the second matrix and access it in a row-wise order.</p>

<p>By editing the program to transpose <code class="highlighter-rouge">m2</code> before calling <code class="highlighter-rouge">mm</code> and run it inside
QEMU with the plugin attached and using the same configuration as previously,
the following data is logged in <code class="highlighter-rouge">matmul.log</code>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>core #, data accesses, data misses, dmiss rate, insn accesses, insn misses, imiss rate
0       4998994        24235           0.4848%  8191937        1009            0.0123%

address, data misses, instruction
0x4000001244 (mm), 16447, movl (%rdi, %rsi, 4), %esi
0x4000001359 (tran), 3994, movl (%rcx, %rdx, 4), %ecx
0x4000001aa7 (main), 257, movl %edx, (%rax, %rcx, 4)
0x4000001a72 (main), 257, movl %ecx, (%rax, %rdx, 4)

...
</code></pre></div></div>

<p>It can be seen that a minor number of misses is generated at transposition time
in <code class="highlighter-rouge">tran</code>. The rest of the matrix multiplication is carried out using the same
procedure but to multiply <code class="highlighter-rouge">m1[i][k]</code> by <code class="highlighter-rouge">m2[j][k]</code>. So <code class="highlighter-rouge">m2</code> is traversed
row-wise and hence utilized cache space much more optimally.</p>

<h3 id="multi-core-caching">Multi-core caching</h3>

<p>The plugin accepts a <code class="highlighter-rouge">cores=N_CORES</code> argument that represents the number of
cores that the plugin must keep track of. Memory accesses generated by excess
threads will be served through the available core caches. The model is an
approximation, as described, and is most-akin to idealized behaviour when the
number of threads generated by the program is less than cores available,
otherwise inter-thread thrashing will invariably occur.</p>

<p>An example usage of the plugin using the <code class="highlighter-rouge">cores</code> argument to use 4 per-core
caches against a multithreaded program:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./qemu-x86_64 $(QEMU_ARGS) \
    -plugin ./contrib/plugins/libcache.so,cores=4 \
    -d plugin \
    -D logfile \
    ./threaded_prog
</code></pre></div></div>

<p>This reports out the following:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>core #, data accesses, data misses, dmiss rate, insn accesses, insn misses, imiss rate
0       76739          4195          5.411666%  242616         1555            0.6409%
1       29029          932           3.211106%  70939          988             1.3927%
2       6218           285           4.511835%  15702          382             2.4328%
3       6608           297           4.411946%  16342          384             2.3498%
sum     118594         5709          4.811139%  345599         3309            0.9575%

...
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>By emulating simple configurations of icache and dcache we can gain insights
into how a working set is utilizing cache memory. Simplicity is sought and L1
cache is emphasized since its under-utilization can be severe to the overall
system performance.</p>

<p>This plugin is made as part of my GSoC participation for the year 2021 under the
mentorship of Alex Bennée.</p>

<p>List of posted patches related to the plugin:</p>

<ul>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/">[PATCH v4 0/5] plugins: New TCG plugin for cache modelling</a></li>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/20210623125458.450462-2-ma.mandourr@gmail.com/">[PATCH v4 1/5] plugins: Added a new cache modelling plugin</a></li>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/20210623125458.450462-3-ma.mandourr@gmail.com/">[PATCH v4 2/5] plugins/cache: Enable cache parameterization</a></li>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/20210623125458.450462-4-ma.mandourr@gmail.com/">[PATCH v4 3/5] plugins/cache: Added FIFO and LRU eviction policies</a></li>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/20210623125458.450462-5-ma.mandourr@gmail.com/#20210628053808.17422-1-ma.mandourr@gmail.com">[PATCH v4 4/5] docs/devel: Added cache plugin to the plugins docs</a></li>
  <li><a href="https://patchew.org/QEMU/20210623125458.450462-1-ma.mandourr@gmail.com/20210623125458.450462-6-ma.mandourr@gmail.com/#20210707092756.414242-1-ma.mandourr@gmail.com">[PATCH v5] MAINTAINERS: Added myself as a reviewer for TCG Plugins</a></li>
  <li><a href="https://patchew.org/QEMU/20210714172151.8494-1-ma.mandourr@gmail.com/20210714172151.8494-2-ma.mandourr@gmail.com/">[PATCH 1/6] plugins/cache: Fixed a bug with destroying FIFO metadata</a></li>
  <li><a href="https://patchew.org/QEMU/20210714172151.8494-1-ma.mandourr@gmail.com/20210714172151.8494-3-ma.mandourr@gmail.com/">[PATCH 2/6] plugins/cache: limited the scope of a mutex lock</a></li>
  <li><a href="https://patchew.org/QEMU/20210714172151.8494-1-ma.mandourr@gmail.com/20210714172151.8494-7-ma.mandourr@gmail.com/">[PATCH 6/6] plugins/cache: Fixed “function decl. is not a prototype” warnings</a></li>
  <li><a href="https://patchew.org/QEMU/20210803151301.123581-1-ma.mandourr@gmail.com/">[PATCH v5 0/2] plugins/cache: multicore cache modelling</a></li>
  <li><a href="https://patchew.org/QEMU/20210803151301.123581-1-ma.mandourr@gmail.com/20210803151301.123581-3-ma.mandourr@gmail.com/">[PATCH v5 1/2] plugins/cache: supported multicore cache modelling</a></li>
</ul>

<p>The first series, (plugins: New TCG plugin for cache modelling), along with the
bug fixes patches are already merged to the QEMU main tree, the remaining
patches are merged to the <a href="https://github.com/stsquad/qemu/tree/plugins/next">plugins/next</a>
tree, awaiting merging to the main tree, since we’re in a release cycle as of
the time of posting.</p>]]></content><author><name>Mahmoud Mandour</name></author><category term="TCG plugins" /><category term="GSOC" /><summary type="html"><![CDATA[Caches are a key way that enables modern CPUs to keep running at full speed by avoiding the need to fetch data and instructions from the comparatively slow system memory. As a result understanding cache behaviour is a key part of performance optimisation.]]></summary></entry></feed>